{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>short</th>\n",
       "      <th>file</th>\n",
       "      <th>script</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>submission_xgb_fold_4.csv</td>\n",
       "      <td>class1,class2,class3,class4,class5,class6,clas...</td>\n",
       "      <td>\\n%%time\\nfrom sklearn import *\\nimport sklear...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id                      short  \\\n",
       "0   3  submission_xgb_fold_4.csv   \n",
       "\n",
       "                                                file  \\\n",
       "0  class1,class2,class3,class4,class5,class6,clas...   \n",
       "\n",
       "                                              script  \n",
       "0  \\n%%time\\nfrom sklearn import *\\nimport sklear...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3, io\n",
    "\n",
    "c = sqlite3.connect('input/dataset01/database.sqlite')\n",
    "df = pd.read_sql('Select * From stacks Where Id=3', c)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission = pd.read_csv(io.StringIO(df['file'][0]))\n",
    "submission.to_csv('submission_xgb_fold_4.csv', index=False) #0.57019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('submission_xgb_fold_4.csv')\n",
    "df2 = pd.read_csv('submission_xgb.csv')\n",
    "df2.columns = [x+'_' if x not in ['ID'] else x for x in df2.columns]\n",
    "blend = pd.merge(df1, df2, how='left', on='ID')\n",
    "for c in df1.columns:\n",
    "   if c != 'ID':\n",
    "       blend[c] = (blend[c] * 0.5)  + (blend[c+'_'] * 0.5)\n",
    "blend = blend[df1.columns]\n",
    "blend.to_csv('blend3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c = sqlite3.connect('input/dataset01/database.sqlite')\n",
    "df = pd.read_sql('Select * From stacks', c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>short</th>\n",
       "      <th>file</th>\n",
       "      <th>script</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NYC_Taxi.zip</td>\n",
       "      <td>[P, K, \u0003, \u0004, \u0014, \u0003, \u0000, \u0000, \b, \u0000, �, q, �, J, �, ...</td>\n",
       "      <td>\\nDATASETS:\\nhttps://data.cityofnewyork.us/Tra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NYC_Taxi_sub_0.382PBL.csv</td>\n",
       "      <td>id,trip_duration\\nid3004672,751.0471190915441\\...</td>\n",
       "      <td>\\nDATASETS:\\nNew York City Taxi Trip Duration ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>submission_xgb_fold_4.csv</td>\n",
       "      <td>class1,class2,class3,class4,class5,class6,clas...</td>\n",
       "      <td>\\n%%time\\nfrom sklearn import *\\nimport sklear...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id                      short  \\\n",
       "0   1               NYC_Taxi.zip   \n",
       "1   2  NYC_Taxi_sub_0.382PBL.csv   \n",
       "2   3  submission_xgb_fold_4.csv   \n",
       "\n",
       "                                                file  \\\n",
       "0  [P, K, \u0003, \u0004, \u0014, \u0003, \u0000, \u0000, \b, \u0000, �, q, �, J, �, ...   \n",
       "1  id,trip_duration\\nid3004672,751.0471190915441\\...   \n",
       "2  class1,class2,class3,class4,class5,class6,clas...   \n",
       "\n",
       "                                              script  \n",
       "0  \\nDATASETS:\\nhttps://data.cityofnewyork.us/Tra...  \n",
       "1  \\nDATASETS:\\nNew York City Taxi Trip Duration ...  \n",
       "2  \\n%%time\\nfrom sklearn import *\\nimport sklear...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>short</th>\n",
       "      <th>file</th>\n",
       "      <th>script</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NYC_Taxi.zip</td>\n",
       "      <td>[P, K, \u0003, \u0004, \u0014, \u0003, \u0000, \u0000, \b, \u0000, �, q, �, J, �, ...</td>\n",
       "      <td>\\nDATASETS:\\nhttps://data.cityofnewyork.us/Tra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NYC_Taxi_sub_0.382PBL.csv</td>\n",
       "      <td>id,trip_duration\\nid3004672,751.0471190915441\\...</td>\n",
       "      <td>\\nDATASETS:\\nNew York City Taxi Trip Duration ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>submission_xgb_fold_4.csv</td>\n",
       "      <td>class1,class2,class3,class4,class5,class6,clas...</td>\n",
       "      <td>\\n%%time\\nfrom sklearn import *\\nimport sklear...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id                      short  \\\n",
       "0   1               NYC_Taxi.zip   \n",
       "1   2  NYC_Taxi_sub_0.382PBL.csv   \n",
       "2   3  submission_xgb_fold_4.csv   \n",
       "\n",
       "                                                file  \\\n",
       "0  [P, K, \u0003, \u0004, \u0014, \u0003, \u0000, \u0000, \b, \u0000, �, q, �, J, �, ...   \n",
       "1  id,trip_duration\\nid3004672,751.0471190915441\\...   \n",
       "2  class1,class2,class3,class4,class5,class6,clas...   \n",
       "\n",
       "                                              script  \n",
       "0  \\nDATASETS:\\nhttps://data.cityofnewyork.us/Tra...  \n",
       "1  \\nDATASETS:\\nNew York City Taxi Trip Duration ...  \n",
       "2  \\n%%time\\nfrom sklearn import *\\nimport sklear...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'\\n%%time\\nfrom sklearn import *\\nimport sklearn\\nimport pandas as pd\\nimport numpy as np\\nimport xgboost as xgb\\n\\ntrain = pd.read_csv(\\'../input/training_variants\\')\\ntest = pd.read_csv(\\'../input/test_variants\\')\\ntrainx = pd.read_csv(\\'../input/training_text\\', sep=\"\\\\|\\\\|\", engine=\\'python\\', header=None, skiprows=1, names=[\"ID\",\"Text\"])\\ntestx = pd.read_csv(\\'../input/test_text\\', sep=\"\\\\|\\\\|\", engine=\\'python\\', header=None, skiprows=1, names=[\"ID\",\"Text\"])\\n\\ntrain = pd.merge(train, trainx, how=\\'left\\', on=\\'ID\\').fillna(\\'\\')\\ny_train = train[\\'Class\\'].values\\ntrain = train.drop([\\'Class\\'], axis=1)\\n\\ntest = pd.merge(test, testx, how=\\'left\\', on=\\'ID\\').fillna(\\'\\')\\npid = test[\\'ID\\'].values\\n\\ndf_all = pd.concat((train, test), axis=0, ignore_index=True)\\ndf_all[\\'Gene_Share\\'] = df_all.apply(lambda r: sum([1 for w in r[\\'Gene\\'].split(\\' \\') if w in r[\\'Text\\'].split(\\' \\')]), axis=1)\\ndf_all[\\'Variation_Share\\'] = df_all.apply(lambda r: sum([1 for w in r[\\'Variation\\'].split(\\' \\') if w in r[\\'Text\\'].split(\\' \\')]), axis=1)\\n\\nfor i in range(56):\\n    df_all[\\'Gene_\\'+str(i)] = df_all[\\'Gene\\'].map(lambda x: str(x[i]) if len(x)>i else \\'\\')\\n    df_all[\\'Variation\\'+str(i)] = df_all[\\'Variation\\'].map(lambda x: str(x[i]) if len(x)>i else \\'\\')\\n\\ngen_var_lst = sorted(list(train.Gene.unique()) + list(train.Variation.unique()))\\nprint(len(gen_var_lst))\\ngen_var_lst = [x for x in gen_var_lst if len(x.split(\\' \\'))==1]\\nprint(len(gen_var_lst))\\ni_ = 0\\n\\nfor gen_var_lst_itm in gen_var_lst:\\n    if i_ % 100 == 0: print(i_)\\n    df_all[\\'GV_\\'+str(gen_var_lst_itm)] = df_all[\\'Text\\'].map(lambda x: str(x).count(str(gen_var_lst_itm)))\\n    i_ += 1\\n\\n#count unique word instances #does not take into consideration documents word may only appear in one doc\\ntrain[\\'Class\\'] = y_train\\nd={}\\nd_unique = {}\\nfor y in train[\\'Class\\'].unique():\\n    print(y)\\n    d[str(y)] = {}\\n    df = train[train[\\'Class\\'] == y].reset_index(drop=True)\\n    for i in range(len(df)):\\n        words = str(df[\\'Text\\'][i]).lower().split(\\' \\')\\n        for w in words:\\n            if w in d[str(y)]:\\n                d[str(y)][w] += 1\\n            else:\\n                d[str(y)][w] = 1\\nfor y1 in train[\\'Class\\'].unique():\\n    words = [x for x in d[str(y1)]]\\n    for y2 in train[\\'Class\\'].unique():\\n        if y2 != y1:\\n            words = [x for x in words if x not in d[str(y2)]]\\n    words = [str(j[1]) for j in sorted([[d[str(y1)][w],w] for w in words], reverse=True)][:10]\\n    d_unique[str(y1)] = words\\n#print(d_unique)\\nfor k in d_unique:\\n    for w in d_unique[k]:\\n        print(k, w)\\n        df_all[\\'SW_\\'+str(k)+\\'_\\'+str(w)] = df_all[\\'Text\\'].map(lambda x: str(x).lower().count(str(w)))\\n        \\nfor c in df_all.columns:\\n    if df_all[c].dtype == \\'object\\':\\n        if c in [\\'Gene\\',\\'Variation\\']:\\n            lbl = preprocessing.LabelEncoder()\\n            df_all[c+\\'_lbl_enc\\'] = lbl.fit_transform(df_all[c].values)  \\n            df_all[c+\\'_len\\'] = df_all[c].map(lambda x: len(str(x)))\\n            df_all[c+\\'_words\\'] = df_all[c].map(lambda x: len(str(x).split(\\' \\')))\\n        elif c != \\'Text\\':\\n            lbl = preprocessing.LabelEncoder()\\n            df_all[c] = lbl.fit_transform(df_all[c].values)\\n        if c==\\'Text\\': \\n            df_all[c+\\'_len\\'] = df_all[c].map(lambda x: len(str(x)))\\n            df_all[c+\\'_words\\'] = df_all[c].map(lambda x: len(str(x).split(\\' \\'))) \\n\\ntrain = df_all.iloc[:len(train)]\\ntest = df_all.iloc[len(train):]\\n\\nclass cust_regression_vals(sklearn.base.BaseEstimator, sklearn.base.TransformerMixin):\\n    def fit(self, x, y=None):\\n        return self\\n    def transform(self, x):\\n        x = x.drop([\\'Gene\\', \\'Variation\\',\\'ID\\',\\'Text\\'],axis=1).values\\n        return x\\n\\nclass cust_txt_col(sklearn.base.BaseEstimator, sklearn.base.TransformerMixin):\\n    def __init__(self, key):\\n        self.key = key\\n    def fit(self, x, y=None):\\n        return self\\n    def transform(self, x):\\n        return x[self.key].apply(str)\\n\\nprint(\\'Pipeline...\\')\\nfp = pipeline.Pipeline([\\n    (\\'union\\', pipeline.FeatureUnion(\\n        n_jobs = -1,\\n        transformer_list = [\\n            (\\'standard\\', cust_regression_vals()),\\n            (\\'pi1\\', pipeline.Pipeline([(\\'Gene\\', cust_txt_col(\\'Gene\\')), (\\'count_Gene\\', feature_extraction.text.CountVectorizer(analyzer=u\\'char\\', ngram_range=(1, 8))), (\\'tsvd1\\', decomposition.TruncatedSVD(n_components=20, n_iter=25, random_state=12))])),\\n            (\\'pi2\\', pipeline.Pipeline([(\\'Variation\\', cust_txt_col(\\'Variation\\')), (\\'count_Variation\\', feature_extraction.text.CountVectorizer(analyzer=u\\'char\\', ngram_range=(1, 8))), (\\'tsvd2\\', decomposition.TruncatedSVD(n_components=20, n_iter=25, random_state=12))])),\\n            (\\'pi3\\', pipeline.Pipeline([(\\'Text\\', cust_txt_col(\\'Text\\')), (\\'tfidf_Text\\', feature_extraction.text.TfidfVectorizer(ngram_range=(1, 2))), (\\'tsvd3\\', decomposition.TruncatedSVD(n_components=50, n_iter=25, random_state=12))]))\\n        ])\\n    )])\\n\\ntrain = fp.fit_transform(train); print(train.shape)\\ntest = fp.transform(test); print(test.shape)\\n\\ny = y_train - 1 #fix for zero bound array\\n\\ndenom = 0\\nfold = 5\\nfor i in range(fold):\\n    params = {\\n        \\'eta\\': 0.03333,\\n        \\'max_depth\\': 4,\\n        \\'objective\\': \\'multi:softprob\\',\\n        \\'eval_metric\\': \\'mlogloss\\',\\n        \\'num_class\\': 9,\\n        \\'seed\\': i,\\n        \\'silent\\': True\\n    }\\n    x1, x2, y1, y2 = model_selection.train_test_split(train, y, test_size=0.18, random_state=i)\\n    watchlist = [(xgb.DMatrix(x1, y1), \\'train\\'), (xgb.DMatrix(x2, y2), \\'valid\\')]\\n    model = xgb.train(params, xgb.DMatrix(x1, y1), 1000,  watchlist, verbose_eval=50, early_stopping_rounds=100)\\n    score1 = metrics.log_loss(y2, model.predict(xgb.DMatrix(x2), ntree_limit=model.best_ntree_limit), labels = list(range(9)))\\n    print(score1)\\n    if denom != 0:\\n        pred = model.predict(xgb.DMatrix(test), ntree_limit=model.best_ntree_limit+80)\\n        preds += pred\\n    else:\\n        pred = model.predict(xgb.DMatrix(test), ntree_limit=model.best_ntree_limit+80)\\n        preds = pred.copy()\\n    denom += 1\\n    submission = pd.DataFrame(pred, columns=[\\'class\\'+str(c+1) for c in range(9)])\\n    submission[\\'ID\\'] = pid\\n    submission.to_csv(\\'submission_xgb_fold_\\'  + str(i) + \\'.csv\\', index=False)\\npreds /= denom\\nsubmission = pd.DataFrame(preds, columns=[\\'class\\'+str(c+1) for c in range(9)])\\nsubmission[\\'ID\\'] = pid\\nsubmission.to_csv(\\'submission_xgb.csv\\', index=False)\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['script'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
